{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hello,TensorFlow!'\n"
     ]
    }
   ],
   "source": [
    "# using tensorflow_gpuenv\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import re\n",
    "\n",
    "hello=tf.constant('Hello,TensorFlow!')\n",
    "\n",
    "sess=tf.Session()\n",
    "\n",
    "print(sess.run(hello))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = {}\n",
    "\n",
    "def clean_word(string):\n",
    "    wordlist = re.findall(r\"[a-z]+\", string.lower())\n",
    "    word = \"\"\n",
    "    if len(wordlist) == 0:\n",
    "        word = \"#NUM\"\n",
    "    else:\n",
    "        word = wordlist[0]\n",
    "    return word\n",
    "    \n",
    "\n",
    "def get_word_counts(data, all_words):\n",
    "    for i in range(len(data)):\n",
    "        currentword = str(data.iloc[i,0])\n",
    "        word = clean_word(currentword)\n",
    "        if word in all_words:\n",
    "            all_words[word] += data.iloc[i,2]\n",
    "        else:\n",
    "            all_words[word] = data.iloc[i,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  word  year  total  pages  books\n",
      "0    #  1574      1      1      1\n",
      "1    #  1584      6      6      1\n",
      "2    #  1614      1      1      1\n",
      "3    #  1631    115    100      1\n",
      "4    #  1632      3      3      1\n",
      "  word  year  total  pages  books\n",
      "0    $  1520      1      1      1\n",
      "1    $  1575     12     12      2\n",
      "2    $  1576      1      1      1\n",
      "3    $  1581      1      1      1\n",
      "4    $  1584     41     38      1\n",
      "     word  year  total  pages  books\n",
      "0  $0.007  1905      1      1      1\n",
      "1  $0.007  1910      2      2      2\n",
      "2  $0.007  1911      3      3      3\n",
      "3  $0.007  1912      6      6      6\n",
      "4  $0.007  1913      4      4      4\n",
      "    word  year  total  pages  books\n",
      "0  $0.00  1751      1      1      1\n",
      "1  $0.00  1782      1      1      1\n",
      "2  $0.00  1834      1      1      1\n",
      "3  $0.00  1848      2      2      2\n",
      "4  $0.00  1849      1      1      1\n",
      "     word  year  total  pages  books\n",
      "0  $0.002  1906      1      1      1\n",
      "1  $0.002  1908      2      2      2\n",
      "2  $0.002  1909      5      5      5\n",
      "3  $0.002  1910      1      1      1\n",
      "4  $0.002  1911      4      4      3\n",
      "   word  year  total  pages  books\n",
      "0  $0.0  1822      1      1      1\n",
      "1  $0.0  1848      3      3      2\n",
      "2  $0.0  1863      1      1      1\n",
      "3  $0.0  1871      1      1      1\n",
      "4  $0.0  1873      1      1      1\n",
      "      word  year  total  pages  books\n",
      "0  $0.0005  1878      1      1      1\n",
      "1  $0.0005  1896      2      2      2\n",
      "2  $0.0005  1906      1      1      1\n",
      "3  $0.0005  1911      1      1      1\n",
      "4  $0.0005  1912      1      1      1\n",
      "  word  year  total  pages  books\n",
      "0   $0  1631      1      1      1\n",
      "1   $0  1658      1      1      1\n",
      "2   $0  1670      1      1      1\n",
      "3   $0  1674      3      3      1\n",
      "4   $0  1682      2      2      1\n",
      "     word  year  total  pages  books\n",
      "0  \"\"\"ê\u0011\"  1520    529     89      1\n",
      "1  \"\"\"ê\u0011\"  1527     22      6      1\n",
      "2  \"\"\"ê\u0011\"  1541    126     14      1\n",
      "3  \"\"\"ê\u0011\"  1574   1326    244      1\n",
      "4  \"\"\"ê\u0011\"  1575   7932    790      2\n",
      "  word  year  total  pages  books\n",
      "0    !  1520     99     57      1\n",
      "1    !  1527      2      2      1\n",
      "2    !  1574     12     12      1\n",
      "3    !  1575     26     19      2\n",
      "4    !  1576     12      8      1\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    filename = \"googlebooks-eng-1M-1gram-20090715-\" + str(i) + \".csv\"\n",
    "    data = pd.read_csv(filename, \n",
    "                       sep = '\\t', \n",
    "                       header = None, \n",
    "                       names = [\"word\",\"year\",\"total\",\"pages\",\"books\"],\n",
    "                       quotechar=None, \n",
    "                       quoting=3,\n",
    "                       encoding = \"ISO-8859-1\")\n",
    "    print(data.head())\n",
    "    get_word_counts(data, all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['prolecanites', 'ensalza', 'villorum', 'cdntico', 'lats', 'preponderating', 'pitent', 'haloing', 'greeloy', 'inverarity', 'freny', 'kontingenz', 'sideopening', 'tahat', 'hypacyris', 'potio', 'nordblom', 'dispensatrix', 'solez', 'callophis', 'wyssehrad', 'soveraign', 'approvec', 'wanamaker', 'convulsiva', 'thougjit', 'renoard', 'gatliffe', 'ochrecoloured', 'demonologic', 'pesotta', 'broodeth', 'posssble', 'gitis', 'versibility', 'pourchasser', 'gelaedde', 'abdou', 'angeliche', 'meliboean', 'acroinon', 'omero', 'symonette', 'chuiing', 'aquclla', 'andrule', 'singleseaters', 'habsburgh', 'gerstfeldt', 'themfelve', 'mctered', 'expilly', 'groynes', 'pertusati', 'dileo', 'mercenier', 'hogsnorton', 'overgraze', 'hauptformen', 'elsenore', 'ofjiirgen', 'anxitty', 'brismar', 'kuenzer', 'frintbd', 'porcher', 'mingusville', 'jiatutes', 'vccchi', 'vorangehende', 'abernathy', 'ascendant', 'houwelingen', 'monotholites', 'kultursprachen', 'proudlock', 'esterias', 'opjxisite', 'pitocchi', 'knoeringen', 'blouiit', 'hydrophytic', 'desideravano', 'concubia', 'parobolic', 'perditio', 'orthologs', 'forgall', 'ipply', 'consulemus', 'porquel', 'pnblifhed', 'essentiels', 'skydrol', 'issuu', 'ergehnisse', 'quesse', 'vorhandenheit', 'repoach', 'ferardo', 'necera', 'seare', 'towneleys', 'goodiness', 'passionner', 'ellicombe', 'eguren', 'ermenric', 'brockett', 'jarasch', 'ferrington', 'fariously', 'sotherland', 'solarcaine', 'shuqualak', 'patts', 'chelagskoi', 'handelsfreiheit', 'limnologischen', 'btex', 'arsenates', 'fomt', 'reklame', 'antinomianisin', 'anuwer', 'oranino', 'arrid', 'farlie', 'schulprogramm', 'distantiate', 'zillebeke', 'nativorum', 'roota', 'purshia', 'chusctts', 'readyest', 'shavel', 'eighteenpeuce', 'lyfr', 'delene', 'toucy', 'konyve', 'ebir', 'gravemen', 'theeoman', 'wananto', 'laborunions', 'grcus', 'vorkt', 'clirnate', 'irrdr', 'amphibologia', 'ammeres', 'lepartments', 'dewes', 'japonski', 'turhulence', 'buddhistic', 'idak', 'tyndarid', 'brunnanburgh', 'chickweed', 'quelling', 'mortel', 'szegedi', 'yajflavalkya', 'albia', 'hintrager', 'proturbatis', 'parkend', 'accumulatively', 'gessner', 'criscrossing', 'wurzburgers', 'ihorc', 'uvcdale', 'cauldshiel', 'deeson', 'vexatioufly', 'spirometer', 'mateni', 'gweirydd', 'tuamotus', 'kneepans', 'wonede', 'dauther', 'mojith', 'dimonourable', 'rnau', 'ewtown', 'eccezioni', 'birjia', 'olcano', 'spending', 'gruicciardini', 'athenie', 'aeeds', 'witchhunters', 'arragou', 'kincaid', 'spsc', 'buggs', 'shebman', 'ipro', 'darmstad', 'debranching', 'lteste', 'inunion', 'archaeocyatha', 'middletoivn', 'aquilecchia', 'parodique', 'trasicor', 'reiated', 'poktical', 'amacher', 'alula', 'unruhiger', 'structureproperty', 'webater', 'kilrie', 'threatei', 'imerovement', 'rput', 'salvad', 'handywomen', 'rcgarde', 'pusan', 'viill', 'krahwinkel', 'catalino', 'setha', 'plainweave', 'reing', 'adjutory', 'larusson', 'thurlston', 'tussim', 'ballett', 'gebrauchlichsten', 'incierto', 'verbage', 'jnited', 'chamba', 'bodypolitic', 'pfitzmann', 'souriaient', 'vefifels', 'thoracocervical', 'ftnish', 'parminder', 'lionfish', 'pasumot', 'chemyn', 'listinct', 'riadh', 'disal', 'admonent', 'eonian', 'rpign', 'rerented', 'bofetadas', 'hreaten', 'enssent', 'crudelior', 'broard', 'maltsberger', 'wakita', 'beachtlich', 'firmily', 'ttco', 'fotts', 'shihabu', 'brcraght', 'huqqa', 'gcstav', 'belna', 'buffinton', 'quxdam', 'castaneum', 'crauley', 'migratione', 'morbose', 'dehold', 'reformiste', 'induxisti', 'cosins', 'karaikkal', 'uphalding', 'goudea', 'astataries', 'morlacchi', 'histoly', 'hermoder', 'temenium', 'eiection', 'ijon', 'fideles', 'byitself', 'ceutrale', 'agello', 'marcbed', 'conchifer', 'unitstress', 'worlhy', 'kawatsu', 'homelessons', 'afora', 'marqucsan', 'sidbury', 'rfte', 'leuchtenden', 'fiiilure', 'nammalwar', 'shils', 'wendover', 'kacanik', 'southron', 'batutsi', 'palu', 'woodsons', 'tochinge', 'shenkar', 'avarizia', 'adina', 'arislote', 'pruski', 'incerta', 'splittings', 'lysinuria', 'retaliator', 'cueurbita', 'ulfric', 'tollbars', 'quadrigemiua', 'soluble', 'coucentration', 'klinteberg', 'szentpetery', 'termx', 'deadeyed', 'skarstedt', 'ferjeant', 'asquiescence', 'varnod', 'llrd', 'benedictionals', 'cestuans', 'releaser', 'bradneld', 'shrinkwrapped', 'atral', 'vocationalization', 'olonez', 'raiied', 'puedese', 'conlideration', 'houzeau', 'specklin', 'lenire', 'rottboell', 'ptolemajus', 'comtessede', 'khrgian', 'hernicans', 'echeverry', 'snmm', 'teleological', 'ooviet', 'kniazei', 'masten', 'ledinko', 'singidon', 'rhynchospermum', 'pelec', 'caretairs', 'arsenale', 'stummem', 'stonepine', 'ccela', 'capitolio', 'fmality', 'mangyongdae', 'ceratopsia', 'uward', 'qvasi', 'levigating', 'disuse', 'paragate', 'pruessen', 'againstone', 'shudder', 'mantchu', 'headuarters', 'eolved', 'conringius', 'impair', 'disentagle', 'tlapujahua', 'vorstel', 'affittance', 'chaldfean', 'halfperiod', 'senatumque', 'nethy', 'refuseheap', 'wynnstay', 'dadap', 'ebility', 'materii', 'chelny', 'subby', 'gunnerby', 'gaietle', 'celebrem', 'sixdigit', 'wftl', 'baori', 'odic', 'stification', 'scavaging', 'manzato', 'pngi', 'ftck', 'sporogonous', 'antenae', 'ishimoto', 'samikon', 'garbhadhana', 'reacli', 'thermomechanics', 'chandrabansi', 'calafornia', 'paoting', 'zeitlang', 'saugeon', 'asteraceae', 'troussel', 'braunschweigische', 'intentionnelle', 'yitzhaki', 'verarbeitet', 'deduite', 'satwas', 'arborca', 'schulreform', 'congiunte', 'aminopyrimidine', 'vthom', 'aminopolycarboxylic', 'brifgau', 'llanidloes', 'restes', 'instruktive', 'brnhmans', 'reserato', 'amsar', 'studd', 'ydsnaya', 'earnef', 'ruffoli', 'coedmon', 'mehivale', 'toeast', 'beingtowards', 'pedaneus', 'sliker', 'homems', 'heriman', 'sackvillc', 'yukino', 'aiguptos', 'zauberey', 'admrs', 'nir', 'imitationof', 'bauking', 'dongue', 'sixteencandle', 'qubit', 'battante', 'shakcfpeare', 'ungravelled', 'bennettitalean', 'slipstream', 'parallelisieren', 'luster', 'pirkis', 'kahatriya', 'kdlam', 'sayin', 'curiosty', 'selfinsurance', 'fareivell', 'woodbnrn', 'garbai', 'incessants', 'recollections', 'manyarched', 'scotchcanadian', 'massala', 'pickshers', 'oxydierten', 'lubmiffion', 'expuition', 'farmgirl', 'percepire', 'ayorinde', 'antithetis', 'nagatoki', 'rewritings', 'usurpatory', 'lycosid', 'warloy', 'substerni', 'slirr', 'ifrt', 'aguzzi', 'hildebr', 'nmcli', 'satak', 'somervillk', 'difadvantagcs', 'democrirus', 'quetry', 'animatae', 'booran', 'jaysingh', 'necessairc', 'breathinghole', 'bardolphs', 'lookdown', 'freqi', 'gallieia', 'soluch', 'capituli', 'asisting', 'fictioning', 'verroux', 'ingrauen', 'beriia', 'undergoe', 'darwinsche', 'dromer', 'bebby', 'smolensko', 'candhar', 'yinchu', 'ductules', 'mufls', 'stowin', 'matrc', 'sereuses', 'llazen', 'inferiptions', 'fufed', 'deeplier', 'iclieved', 'kheima', 'amaltheia', 'diftusivity', 'perfectionnees', 'alfridi', 'viseur', 'synchronical', 'utrafque', 'emial', 'redounded', 'eail', 'panathenean', 'glammiss', 'spectantia', 'btus', 'indoaustralian', 'locupletissimis', 'briva', 'accedendum', 'rurai', 'catwalks', 'ooedience', 'unwholefomenefs', 'physicians', 'daweses', 'stocket', 'avonbridge', 'vasn', 'spirituul', 'taylortown', 'ramblings', 'foightin', 'fiama', 'allemagna', 'homeh', 'multam', 'jaceas', 'fiwi', 'jiiven', 'crowdedness', 'rhyncospermum', 'acusilaus', 'mitsutaro', 'asterida', 'feretrum', 'kntercd', 'wortemberg', 'jugge', 'tandya', 'infof', 'deckred', 'champlev', 'affurc', 'euippe', 'stagehouse', 'reasors', 'fotassii', 'iniciativa', 'pcinting', 'electroreceptor', 'bellhowell', 'ragosta', 'bissago', 'journay', 'viedemann', 'tamila', 'impester', 'mortit', 'conditionand', 'eingebettet', 'ulandi', 'aliif', 'radiationes', 'deshayesii', 'imajo', 'assocs', 'edtt', 'antonie', 'importunacy', 'jyapu', 'igured', 'lnder', 'durcharbeitung', 'crui', 'calhoux', 'idioma', 'retulerunt', 'pargent', 'contcmp', 'kosyrev', 'artridges', 'aclds', 'mizzeh', 'gueit', 'rabast', 'grve', 'trince', 'ovenfors', 'electropneumatic', 'khythm', 'gloominess', 'ocrity', 'aitohison', 'nngen', 'firginie', 'obrenovic', 'xxvi', 'sconyers', 'transcurrit', 'mobilised', 'impoveriflied', 'ressurrected', 'domani', 'gfactor', 'obrutum', 'theorien', 'scoole', 'hephaistia', 'phosphoramide', 'egcean', 'urfelves', 'solankis', 'walloppers', 'electrorate', 'metatheoretically', 'areso', 'kokke', 'lacrolx', 'polemoniace', 'alamode', 'masland', 'wiegen', 'silvcstri', 'richens', 'reputation', 'muson', 'kulite', 'otltcr', 'banck', 'acomodate', 'salusalu', 'phyllopteris', 'anacreontica', 'momths', 'vaying', 'semmelweis', 'magnetometric', 'cnlte', 'unifonnly', 'duft', 'fumptuoufnefs', 'exereised', 'presurgically', 'fcbeme', 'hall', 'phygelius', 'nposition', 'perodicticus', 'workor', 'whicl', 'exclulion', 'instunt', 'radova', 'epinoi', 'gtem', 'apapa', 'precollected', 'irganox', 'magislrates', 'vvilhelm', 'mosaisk', 'voswinkel', 'suini', 'dryopians', 'cruciant', 'abounded', 'pescaderia', 'cabranes', 'omniformity', 'extratubal', 'teatimes', 'ruiued', 'vandyked', 'placys', 'onjuly', 'millimeter', 'stijf', 'itemset', 'tepor', 'janmis', 'itieal', 'daiquiri', 'fittri', 'maholio', 'bacteriology', 'transgariep', 'vorwort', 'guingelot', 'dazincourt', 'impoffibie', 'bagoariorum', 'visitiug', 'observatioun', 'willinghams', 'firststage', 'unordained', 'aimn', 'tapsters', 'waddars', 'ouou', 'improvised', 'elati', 'comminuting', 'goertler', 'immigation', 'srinivasacharya', 'ntinople', 'bailcy', 'leadenly', 'lygosoma', 'hornnes', 'simanau', 'dwain', 'kischer', 'cdj', 'diamondlike', 'sapl', 'vengeace', 'trimbakrao', 'intoxic', 'pdet', 'herraduras', 'impref', 'selfactualizers', 'domovoy', 'usto', 'pilferable', 'fceni', 'ytteh', 'berzeugender', 'mlechcha', 'cadastral', 'kesab', 'shekhawati', 'counterappeals', 'underquote', 'matler', 'calculati', 'beecke', 'dashkov', 'recoined', 'adulescentulum', 'phrenia', 'hidinghole', 'retrapping', 'hombu', 'russdl', 'lueben', 'gattopardo', 'theqry', 'violetgreen', 'landlebens', 'yiik', 'pfay', 'picarooning', 'monpa', 'soxo', 'overperfect', 'isentitled', 'kenwigs', 'applicaple', 'obvenire', 'tacuisse', 'brokenburn', 'bementioned', 'noevus', 'antevertere', 'ferk', 'tolerative', 'korrelationen', 'stalagma', 'retrench', 'aearch', 'estacidn', 'enganado', 'renssalear', 'sholde', 'cognoissons', 'vivell', 'mispagination', 'patellam', 'taekle', 'longheed', 'tuberculeuse', 'lokhart', 'oberlegungen', 'mosj', 'morfologisk', 'staatsvertrage', 'sensui', 'earthborne', 'introds', 'surveyer', 'veit', 'compliquer', 'fingerlike', 'allendale', 'eternumque', 'medcalf', 'citadins', 'cervicali', 'theirlife', 'hoine', 'chafto', 'lachit', 'regurgi', 'decifiye', 'mtot', 'stingeth', 'inverseness', 'preacht', 'vodrey', 'fiks', 'shawlands', 'chaldaeae', 'ieys', 'despar', 'guppyi', 'bosby', 'diugwall', 'loskeletal', 'gnstave', 'pumie', 'scindhia', 'aforefaide', 'readingand', 'cressel', 'florismarte', 'ureae', 'tradef', 'lisanne', 'lumbai', 'smelter', 'impetunt', 'adsit', 'texos', 'epera', 'dyzenhaus', 'viuette', 'longexperience', 'nemic', 'bioaugmentation', 'fulli', 'prepoffeffed', 'vcle', 'ariva', 'lhcy', 'sholten', 'onecompartment', 'trophoneurosis', 'nonbreeding', 'eggshaped', 'ambier', 'thanfrom', 'recogidas', 'annweiler', 'quadratisch', 'nervb', 'neckof', 'bulmer', 'dilapidating', 'macigno', 'rousselon', 'instinctive', 'redialing', 'oninino', 'lind', 'ismailian', 'grunkemeier', 'tumpty', 'mickery', 'amorigft', 'visumque', 'veighing', 'kopais', 'kalinovsky', 'rccoinage', 'jailyard', 'coffce', 'bergerie', 'thelrish', 'kamieriski', 'grandiosas', 'bauparaha', 'impliquerait', 'llexham', 'rawleighe', 'magdelein', 'bbattie', 'insignifiante', 'vessiot', 'zarja', 'iuus', 'begad', 'livmg', 'pupillos', 'arminianism', 'partridgeberries', 'avtien', 'socthet', 'hereditairement', 'oncise', 'ptps', 'aconteus', 'deniec', 'frqnt', 'dor', 'cuui', 'tessaron', 'urokon', 'pnpilla', 'poeniteat', 'algarobas', 'kutschuk', 'tinel', 'unstedfast', 'thannc', 'microcalcification', 'dipigne', 'satvahana', 'mayanist', 'issey', 'lectun', 'svell', 'bentinok', 'fecondite', 'prenties', 'backoffice', 'schilthorn', 'positives', 'moralitat', 'agricolo']\n"
     ]
    }
   ],
   "source": [
    "keys = random.sample(list(all_words), 1000)\n",
    "print(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2017739"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([995.,   2.,   1.,   0.,   1.,   0.,   0.,   0.,   0.,   1.]),\n",
       " array([4.0000000e+01, 8.3357330e+05, 1.6671066e+06, 2.5006399e+06,\n",
       "        3.3341732e+06, 4.1677065e+06, 5.0012398e+06, 5.8347731e+06,\n",
       "        6.6683064e+06, 7.5018397e+06, 8.3353730e+06]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "counts = []\n",
    "for word in keys:\n",
    "    count = all_words[word]\n",
    "    counts.append(count)\n",
    "    if(count>1e7):\n",
    "        print(word)\n",
    "        print(count)\n",
    "plt.hist(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11699 words\n",
      "appearing 103341534423 times\n",
      "others appear 7656749615 times\n"
     ]
    }
   ],
   "source": [
    "top_words = {}\n",
    "word_to_index = {}\n",
    "index_to_word = {}\n",
    "cutoff = 5e5\n",
    "top_word_count = 0\n",
    "extra_word_count = 0\n",
    "i = 0\n",
    "for word, count in all_words.items():\n",
    "    if(count>cutoff):\n",
    "        top_words[word] = count\n",
    "        top_word_count += count\n",
    "        word_to_index[word] = i\n",
    "        index_to_word[i] = word\n",
    "        i += 1\n",
    "    else:\n",
    "        extra_word_count += count\n",
    "        \n",
    "print(str(len(top_words)) + \" words\")\n",
    "print(\"appearing \" + str(top_word_count) + \" times\")\n",
    "print(\"others appear \" + str(extra_word_count) + \" times\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9310192073565864\n"
     ]
    }
   ],
   "source": [
    "print(top_word_count/(extra_word_count + top_word_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to Mikolov, the best results from skipgram came from training with a loss function of negative sampling with k=15. This means using:\n",
    "tf.nn.sampled_softmax_loss(\n",
    "    weights = [num classes x dim],\n",
    "    biases = [num_classes],\n",
    "    labels = one-hot word vector for a single context word\n",
    "    inputs = one quarter of the output,\n",
    "    num_sampled = 15,\n",
    "    num_classes = vocab size,\n",
    "    num_true=1,\n",
    "    sampled_values=None,\n",
    "    remove_accidental_hits=True,\n",
    "    partition_strategy='div',\n",
    "    name='sampled_softmax_loss',\n",
    "    seed=None\n",
    ")\n",
    "source: https://www.tensorflow.org/api_docs/python/tf/nn/sampled_softmax_loss\n",
    "\n",
    "if mode == \"train\":\n",
    "  loss = tf.nn.sampled_softmax_loss(\n",
    "      weights=weights,\n",
    "      biases=biases,\n",
    "      labels=labels,\n",
    "      inputs=inputs,\n",
    "      ...,\n",
    "      partition_strategy=\"div\")\n",
    "elif mode == \"eval\":\n",
    "  logits = tf.matmul(inputs, tf.transpose(weights))\n",
    "  logits = tf.nn.bias_add(logits, biases)\n",
    "  labels_one_hot = tf.one_hot(labels, n_classes)\n",
    "  loss = tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "      labels=labels_one_hot,\n",
    "      logits=logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From hw4:\n",
    "embed_shape = [self.V, self.H]\n",
    "self.W_in_ = tf.get_variable(\"W_in\", embed_shape, initializer=tf.initializers.random_uniform(minval=-1.0,maxval=1.0))\n",
    "x_in_ = tf.nn.embedding_lookup(self.W_in_, self.input_w_)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "vocab_size = len(top_words) + 1\n",
    "embed_dim = 300\n",
    "neg_samples = 15\n",
    "learning_rate_ = .001\n",
    "W_e_ = tf.get_variable(\"W_e_\", \n",
    "                       [vocab_size, embed_dim], \n",
    "                       initializer = tf.initializers.random_uniform(minval=-1.0,\n",
    "                                                                    maxval=1.0, \n",
    "                                                                    seed = 47))\n",
    "\n",
    "train_loss_ = tf.reduce_mean(tf.nn.sampled_softmax_loss(\n",
    "    weights =probably just identity? or W_e_ transpose?,\n",
    "    biases = probably just all zeros?,\n",
    "    labels = embedded context word,\n",
    "    inputs = embedded input word,\n",
    "    num_sampled = neg_samples,\n",
    "    num_classes = vocab_size,\n",
    "    num_true=1)) for each context word\n",
    " \n",
    "# Define optimizer and training op\n",
    "        \n",
    "train_step_ = tf.train.AdamOptimizer(learning_rate=learning_rate_).minimize(train_loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(top_words) + 1\n",
    "embed_dim = 300\n",
    "neg_samples = 15\n",
    "learning_rate_ = .001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from 1520 to 2008\n"
     ]
    }
   ],
   "source": [
    "maxyear = max(data.year)\n",
    "minyear = min(data.year)\n",
    "\n",
    "decades = range(minyear - minyear%10, maxyear, 10)\n",
    "\n",
    "W_e_ = {}\n",
    "\n",
    "print(\"Data from \" + str(minyear) + \" to \" + str(maxyear))\n",
    "for decade in decades:\n",
    "    # initialize embeddings\n",
    "    with tf.variable_scope(tf.get_variable_scope(), reuse=tf.AUTO_REUSE): \n",
    "        W_e_[decade] = tf.get_variable(\"W_e_\" + str(i), \n",
    "                       [vocab_size, embed_dim], \n",
    "                       initializer = tf.initializers.random_uniform(minval=-1.0,\n",
    "                                                                    maxval=1.0, \n",
    "                                                                    seed = 47))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "follow the directions in https://www.tensorflow.org/tutorials/representation/word2vec to get skipgram model running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_center_ = {}\n",
    "v_context_0_= {}\n",
    "v_context_1_ = {}\n",
    "v_context_3_ = {}\n",
    "v_context_4_ = {}\n",
    "train_loss_ = {}\n",
    "train_step_ = {}\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "center_word_ids_ = tf.placeholder(tf.int32, [batch_size])\n",
    "context_word_id_0_ = tf.placeholder(tf.int32, [batch_size,1])\n",
    "context_word_id_1_ = tf.placeholder(tf.int32, [batch_size,1])\n",
    "context_word_id_3_ = tf.placeholder(tf.int32, [batch_size,1])\n",
    "context_word_id_4_ = tf.placeholder(tf.int32, [batch_size,1])\n",
    "\n",
    "for decade in decades:\n",
    "\n",
    "    v_center_[decade] = tf.nn.embedding_lookup(W_e_[decade], center_word_ids_)\n",
    "    v_context_0_[decade] = tf.nn.embedding_lookup(W_e_[decade], context_word_id_0_)\n",
    "    v_context_1_[decade] = tf.nn.embedding_lookup(W_e_[decade], context_word_id_1_)\n",
    "    v_context_3_[decade] = tf.nn.embedding_lookup(W_e_[decade], context_word_id_3_)\n",
    "    v_context_4_[decade] = tf.nn.embedding_lookup(W_e_[decade], context_word_id_4_)\n",
    "    \n",
    "    train_loss_[decade] = tf.add(tf.add(tf.reduce_mean(tf.nn.sampled_softmax_loss(\n",
    "        weights = W_e_[decade],\n",
    "        biases = tf.zeros([embed_dim]),\n",
    "        labels = context_word_id_0_,\n",
    "        inputs = v_center_[decade],\n",
    "        num_sampled = neg_samples,\n",
    "        num_classes = vocab_size,\n",
    "        num_true=1)),\n",
    "                                        tf.reduce_mean(tf.nn.sampled_softmax_loss(\n",
    "        weights = W_e_[decade],\n",
    "        biases = tf.zeros([embed_dim]),\n",
    "        labels = context_word_id_1_,\n",
    "        inputs = v_center_[decade],\n",
    "        num_sampled = neg_samples,\n",
    "        num_classes = vocab_size,\n",
    "        num_true=1))),\n",
    "                                 tf.add(tf.reduce_mean(tf.nn.sampled_softmax_loss(\n",
    "        weights = W_e_[decade],\n",
    "        biases = tf.zeros([embed_dim]),\n",
    "        labels = context_word_id_3_,\n",
    "        inputs = v_center_[decade],\n",
    "        num_sampled = neg_samples,\n",
    "        num_classes = vocab_size,\n",
    "        num_true=1)),\n",
    "                                        tf.reduce_mean(tf.nn.sampled_softmax_loss(\n",
    "        weights = W_e_[decade],\n",
    "        biases = tf.zeros([embed_dim]),\n",
    "        labels = context_word_id_4_,\n",
    "        inputs = v_center_[decade],\n",
    "        num_sampled = neg_samples,\n",
    "        num_classes = vocab_size,\n",
    "        num_true=1))))\n",
    "                                        \n",
    "    \n",
    "    train_step_[decade] = tf.train.AdamOptimizer(learning_rate=learning_rate_).minimize(train_loss_[decade])\n",
    "\n",
    "init=tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_to_decade_dict(filename, wordindices, decades):\n",
    "    decade_dicts = {}\n",
    "    for decade in decades:\n",
    "        # initialize dictionary\n",
    "        decade_dicts[decade] = {}\n",
    "        \n",
    "    data = pd.read_csv(filename, \n",
    "                       sep = '\\t', \n",
    "                       header = None, \n",
    "                       names = [\"words\",\"year\",\"total\",\"pages\",\"books\"],\n",
    "                       quotechar=None, \n",
    "                       quoting=3,\n",
    "                       encoding = \"ISO-8859-1\")\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        # get and split words, convert them to indices \n",
    "        wordlist = str(data.iloc[i,0]).lower().split(\" \")\n",
    "        indexlist = [vocab_size - 1,vocab_size -1,vocab_size -1,vocab_size -1,vocab_size -1]\n",
    "        for i in range(len(wordlist)):\n",
    "            currentword = wordlist[i]\n",
    "            cleanword = clean_word(currentword)\n",
    "            if(cleanword in wordindices):\n",
    "                indexlist[i] = wordindices[cleanword]\n",
    "        indextuple = tuple(indexlist)\n",
    "        \n",
    "        year = data.iloc[i,1]\n",
    "        decade = year - year % 10\n",
    "        \n",
    "        count = data.iloc[i,2]\n",
    "        \n",
    "        if(indextuple in decade_dicts[decade]):\n",
    "            decade_dicts[decade][indextuple] += count\n",
    "        else:\n",
    "            decade_dicts[decade][indextuple] = count\n",
    "    return decade_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              words  year  total  pages  books\n",
      "0  \"! \"\" \"\" He got\"  1846      1      1      1\n",
      "1  \"! \"\" \"\" He got\"  1855      1      1      1\n",
      "2  \"! \"\" \"\" He got\"  1856      1      1      1\n",
      "3  \"! \"\" \"\" He got\"  1857      1      1      1\n",
      "4  \"! \"\" \"\" He got\"  1859      1      1      1\n"
     ]
    }
   ],
   "source": [
    "#sample ngram data:\n",
    "\n",
    "ngram_file = \"googlebooks-eng-1M-5gram-20090715-0.csv\"\n",
    "ngram_data = pd.read_csv(ngram_file, \n",
    "                       sep = '\\t', \n",
    "                       header = None, \n",
    "                       names = [\"words\",\"year\",\"total\",\"pages\",\"books\"],\n",
    "                       quotechar=None, \n",
    "                       quoting=3,\n",
    "                       encoding = \"ISO-8859-1\")\n",
    "print(ngram_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get ngram file\n",
    "\n",
    "decade_dicts = ngram_to_decade_dict(ngram_file, word_to_index, decades)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run skipgram for each decade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize weights the same for every decade\n",
    "# run training for each decade\n",
    "# how to deal with counts? ideally, randomly disperse instances of an ngram throughout training\n",
    "# current solutions: random.choices()\n",
    "# issue: may not see every line. but this is proportional to frequency so idgaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45.04714\n",
      "19.22774\n",
      "18.103184\n",
      "15.629042\n",
      "14.368801\n",
      "11.397928\n",
      "16.756271\n",
      "8.793213\n",
      "9.344069\n",
      "10.537293\n",
      "8.14793\n",
      "9.627025\n",
      "6.620831\n",
      "5.636737\n",
      "7.3352003\n",
      "6.002255\n",
      "14.384178\n",
      "6.1317816\n",
      "8.818291\n",
      "5.270791\n",
      "6.474782\n",
      "3.3689694\n",
      "5.4133344\n",
      "8.179918\n",
      "8.133219\n",
      "4.3990517\n",
      "6.0042486\n",
      "4.1562433\n",
      "5.395422\n",
      "3.7149982\n",
      "8.076654\n",
      "5.6479015\n",
      "9.085486\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-1974e6ce5f3b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m     traindata = random.choices(list(decade_dicts[decade].keys()), \n\u001b[0;32m      8\u001b[0m                                \u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecade_dicts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdecade\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m                                k=batch_size)\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     feed_dict = {\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "max_iter = 50000\n",
    "decade = 1850\n",
    "session = tf.Session()\n",
    "session.run(init)\n",
    "\n",
    "for iteration in range(max_iter):\n",
    "    traindata = random.choices(list(decade_dicts[decade].keys()), \n",
    "                               weights = list(decade_dicts[decade].values()), \n",
    "                               k=batch_size)\n",
    "    \n",
    "    feed_dict = {\n",
    "        context_word_id_0_ :  np.array([x[0] for x in traindata]).reshape((batch_size,1)),\n",
    "        context_word_id_1_ :  np.array([x[1] for x in traindata]).reshape((batch_size,1)),\n",
    "        center_word_ids_ : [x[2] for x in traindata],\n",
    "        context_word_id_3_ :  np.array([x[3] for x in traindata]).reshape((batch_size,1)),\n",
    "        context_word_id_4_ :  np.array([x[4] for x in traindata]).reshape((batch_size,1))\n",
    "    }\n",
    "\n",
    "    ops = [train_loss_[decade], train_step_[decade]]        \n",
    "\n",
    "    cost, _ = session.run(ops, feed_dict)\n",
    "    if(iteration % 1000 == 0):\n",
    "        print(cost)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter = 10000\n",
    "filelist = [\"googlebooks-eng-1M-5gram-20090715-1.csv\",\n",
    "           \"googlebooks-eng-1M-5gram-20090715-2.csv\",\n",
    "           \"googlebooks-eng-1M-5gram-20090715-3.csv\",\n",
    "           \"googlebooks-eng-1M-5gram-20090715-4.csv\",\n",
    "            \"googlebooks-eng-1M-5gram-20090715-5.csv\",\n",
    "            \"googlebooks-eng-1M-5gram-20090715-6.csv\"\n",
    "           ]\n",
    "for filename in filelist:\n",
    "    decade_dicts = ngram_to_decade_dict(filename, wordindices, decades)\n",
    "    for decade in decades:\n",
    "        num_ngrams = len(decade_dicts[decade])\n",
    "        if(num_ngrams >= batch_size):\n",
    "            print(\"Training decade \" + str(decade) + \" with data from \" + filename)\n",
    "            max_decade_iter = min(max_iter, num_ngrams)\n",
    "            for iteration in range(max_decade_iter):\n",
    "                traindata = random.choices(list(decade_dicts[decade].keys()), \n",
    "                                           weights = list(decade_dicts[decade].values()), \n",
    "                                           k=batch_size)\n",
    "\n",
    "                feed_dict = {\n",
    "                    context_word_id_0_ :  np.array([x[0] for x in traindata]).reshape((batch_size,1)),\n",
    "                    context_word_id_1_ :  np.array([x[1] for x in traindata]).reshape((batch_size,1)),\n",
    "                    center_word_ids_ : [x[2] for x in traindata],\n",
    "                    context_word_id_3_ :  np.array([x[3] for x in traindata]).reshape((batch_size,1)),\n",
    "                    context_word_id_4_ :  np.array([x[4] for x in traindata]).reshape((batch_size,1))\n",
    "                }\n",
    "\n",
    "                ops = [train_loss_[decade], train_step_[decade]]        \n",
    "\n",
    "                cost, _ = session.run(ops, feed_dict)\n",
    "                if(iteration % 1000 == 0):\n",
    "                    print(cost)\n",
    "            \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "graph results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
